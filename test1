import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load data
base_path = "C:/Users/alexa/OneDrive/Desktop/Coding/DTSC/housing regressio"
train = pd.read_csv(os.path.join(base_path, "train.csv"))
test = pd.read_csv(os.path.join(base_path, "test.csv"))

# Log-transform target
train["SalePrice"] = np.log1p(train["SalePrice"])

# Combine datasets for consistent preprocessing
all_data = pd.concat([train.drop("SalePrice", axis=1), test], axis=0)

# Feature engineering
all_data["TotalSF"] = all_data["TotalBsmtSF"] + all_data["1stFlrSF"] + all_data["2ndFlrSF"]
all_data["Age"] = all_data["YrSold"] - all_data["YearBuilt"]
all_data["RemodAge"] = all_data["YrSold"] - all_data["YearRemodAdd"]

# Handle missing values
num_cols = all_data.select_dtypes(include=[np.number]).columns
cat_cols = all_data.select_dtypes(include=["object"]).columns

all_data[num_cols] = all_data[num_cols].fillna(all_data[num_cols].median())
all_data[cat_cols] = all_data[cat_cols].fillna("Missing")

# One-hot encoding
all_data = pd.get_dummies(all_data, columns=cat_cols, drop_first=True)

# Split back
X_train = all_data[:train.shape[0]]
X_test = all_data[train.shape[0]:]
y_train = train["SalePrice"]

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ridge Regression with CV
alphas = [0.01, 0.1, 1.0, 10.0, 100.0]
ridge = RidgeCV(alphas=alphas, scoring="neg_mean_squared_error", cv=10)
ridge.fit(X_train_scaled, y_train)

# Cross-validated score
cv_rmse = np.sqrt(-cross_val_score(ridge, X_train_scaled, y_train, scoring="neg_mean_squared_error", cv=10)).mean()
print("CV RMSE:", cv_rmse)
print("Best Alpha:", ridge.alpha_)

# Coefficient analysis
coef_df = pd.Series(ridge.coef_, index=X_train.columns).sort_values(key=np.abs, ascending=False).head(20)

# Plot top influential features
plt.figure(figsize=(10, 6))
sns.barplot(x=coef_df.values, y=coef_df.index)
plt.title("Top 20 Influential Features on Log SalePrice (Ridge Regression)")
plt.xlabel("Coefficient Value")
plt.tight_layout()
plt.show()

# Create predictions and inverse log transform
ridge_preds = np.expm1(ridge.predict(X_test_scaled))
submission = pd.DataFrame({"Id": test["Id"], "SalePrice": ridge_preds})
submission_path = os.path.join(base_path, "ridge_submission.csv")
submission.to_csv(submission_path, index=False)
print(f"Submission file saved to {submission_path}")

# Heatmap of feature correlations
plt.figure(figsize=(12, 10))
top_corr_features = train.corr()["SalePrice"].abs().sort_values(ascending=False).head(20).index
sns.heatmap(train[top_corr_features].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Top Correlated Features with SalePrice")
plt.tight_layout()
plt.show()

# Residual plot
y_train_pred = ridge.predict(X_train_scaled)
residuals = y_train - y_train_pred
plt.figure(figsize=(8, 5))
plt.scatter(y_train_pred, residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residuals vs Predicted Values")
plt.xlabel("Predicted Log SalePrice")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

# Heatmap of feature correlations
plt.figure(figsize=(12, 10))
top_corr_features = train.corr()["SalePrice"].abs().sort_values(ascending=False).head(20).index
sns.heatmap(train[top_corr_features].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Top Correlated Features with SalePrice")
plt.tight_layout()
plt.show()

# Residual plot
y_train_pred = ridge.predict(X_train_scaled)
residuals = y_train - y_train_pred
plt.figure(figsize=(8, 5))
plt.scatter(y_train_pred, residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residuals vs Predicted Values")
plt.xlabel("Predicted Log SalePrice")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()
